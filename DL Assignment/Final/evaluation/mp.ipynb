{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Proejct: Text-guided to Image Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the MP score **</font> so that TAs can grade the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append('..')\n",
    "from evaluation.model import CNN_ENCODER, RNN_ENCODER\n",
    "from utils.data_utils import CUBDataset\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda')\n",
    "\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "cfg_from_file('../cfg/eval_birds.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Returns cosine similarity between x1 and x2, computed along dim\n",
    "    \"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = CNN_ENCODER(256)\n",
    "state_dict = torch.load('./sim_models/bird/image_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "image_encoder.load_state_dict(state_dict)\n",
    "for p in image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load image encoder')\n",
    "image_encoder.eval()\n",
    "\n",
    "# load the image encoder model to obtain the latent feature of the real caption\n",
    "text_encoder = RNN_ENCODER(5450, nhidden=256)\n",
    "state_dict = torch.load('./sim_models/bird/text_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "text_encoder.load_state_dict(state_dict)\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load text encoder')\n",
    "text_encoder.eval()\n",
    "\n",
    "image_encoder = image_encoder.to(device)\n",
    "text_encoder = text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128))\n",
    "])\n",
    "\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, transform=transform, split='test', eval_mode=True)\n",
    "\n",
    "print(f'\\ttest data directory:\\n{test_dataset.split_dir}\\n')\n",
    "print(f'\\t# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "print(f'\\texample of filename of test image:{test_dataset.filenames[0]}\\n')\n",
    "print(f'\\texample of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "print(f'\\t# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "print(f'\\t# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                                              drop_last=False, shuffle=False, num_workers=int(cfg.WORKERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP_list = []\n",
    "DIFF_list = []\n",
    "SIM_list = []\n",
    "for data in test_dataloader:\n",
    "    imgs = data['img'][-1].to(device)\n",
    "    gen_imgs = data['gen_img'][-1].to(device)\n",
    "    captions = data['caps']\n",
    "    captions_lens = data['cap_len']\n",
    "    class_ids = data['cls_id']\n",
    "    keys = data['key']\n",
    "    sentence_idx = data['sent_ix']\n",
    "\n",
    "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_lens, 0, True)\n",
    "    captions = captions[sorted_cap_indices].squeeze()\n",
    "    if data['caps'].size(0) == 1:\n",
    "        captions = captions.unsqueeze(0)\n",
    "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
    "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
    "\n",
    "    if cfg.CUDA:\n",
    "        captions = captions.to(device)\n",
    "        sorted_cap_lens = sorted_cap_lens.to(device)\n",
    "\n",
    "    hidden = text_encoder.init_hidden(captions.size(0))\n",
    "    _, sent_emb = text_encoder(captions, sorted_cap_lens, hidden)\n",
    "\n",
    "    _, sent_code = image_encoder(imgs)\n",
    "    _, gen_sent_code = image_encoder(gen_imgs)\n",
    "\n",
    "    sim = cosine_similarity(gen_sent_code, sent_emb)\n",
    "    l1 = torch.abs(imgs - gen_imgs)\n",
    "    diff = torch.mean(l1.view(l1.size(0), -1), dim=1)\n",
    "    mp = (1 - diff) * sim\n",
    "\n",
    "    MP_list.append(mp.detach().cpu().numpy())\n",
    "    DIFF_list.append(diff.detach().cpu().numpy())\n",
    "    SIM_list.append(sim.detach().cpu().numpy())\n",
    "\n",
    "MP_array = np.concatenate(MP_list, axis=0)\n",
    "DIFF_array = np.concatenate(DIFF_list, axis=0)\n",
    "SIM_array = np.concatenate(SIM_list, axis=0)\n",
    "print('# images for evaluation:', len(MP_array))\n",
    "print('mean:', \"%.6f\" % np.mean(MP_array))\n",
    "\n",
    "np.savez(cfg.MP_FILE, mp=MP_array, diff=DIFF_array, sim=SIM_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:class36]",
   "language": "python",
   "name": "conda-env-class36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
