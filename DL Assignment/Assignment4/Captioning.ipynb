{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"4.Captioning.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"bOWXlQoIvpWD","colab_type":"code","outputId":"d5bd0894-7eb7-4fbf-8849-adfdf2283012","executionInfo":{"status":"ok","timestamp":1582528420496,"user_tz":-540,"elapsed":4609,"user":{"displayName":"Jenny Mok","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBx6WjQhn61jI2o2RUoma2WrRvQIP2K7I8ROHvF7A=s64","userId":"14608516416463645410"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pip install scipy==1.1.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.17.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-D3NfS70vrYf","colab_type":"code","outputId":"81e0fff3-deb6-4f03-9054-0d6a70652dff","executionInfo":{"status":"ok","timestamp":1582528868663,"user_tz":-540,"elapsed":452744,"user":{"displayName":"Jenny Mok","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBx6WjQhn61jI2o2RUoma2WrRvQIP2K7I8ROHvF7A=s64","userId":"14608516416463645410"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["!git clone https://github.com/nutellamok/helper.git\n","!wget http://data.snu.ac.kr/caption_dataset.tar.gz\n","!wget http://data.snu.ac.kr/test_image.tar.gz"],"execution_count":2,"outputs":[{"output_type":"stream","text":["fatal: destination path 'helper' already exists and is not an empty directory.\n","--2020-02-24 07:13:39--  http://data.snu.ac.kr/caption_dataset.tar.gz\n","Resolving data.snu.ac.kr (data.snu.ac.kr)... 147.46.112.208\n","Connecting to data.snu.ac.kr (data.snu.ac.kr)|147.46.112.208|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 319715889 (305M) [application/x-gzip]\n","Saving to: ‘caption_dataset.tar.gz.1’\n","\n","caption_dataset.tar 100%[===================>] 304.90M  1.21MB/s    in 7m 12s  \n","\n","2020-02-24 07:20:52 (723 KB/s) - ‘caption_dataset.tar.gz.1’ saved [319715889/319715889]\n","\n","--2020-02-24 07:20:53--  http://data.snu.ac.kr/test_image.tar.gz\n","Resolving data.snu.ac.kr (data.snu.ac.kr)... 147.46.112.208\n","Connecting to data.snu.ac.kr (data.snu.ac.kr)|147.46.112.208|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1260935 (1.2M) [application/x-gzip]\n","Saving to: ‘test_image.tar.gz.1’\n","\n","test_image.tar.gz.1 100%[===================>]   1.20M   205KB/s    in 7.2s    \n","\n","2020-02-24 07:21:02 (172 KB/s) - ‘test_image.tar.gz.1’ saved [1260935/1260935]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qwS6GzARO67o","colab_type":"code","outputId":"90a2cef1-e742-4959-961a-2057da476b18","executionInfo":{"status":"ok","timestamp":1582528878165,"user_tz":-540,"elapsed":462199,"user":{"displayName":"Jenny Mok","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBx6WjQhn61jI2o2RUoma2WrRvQIP2K7I8ROHvF7A=s64","userId":"14608516416463645410"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["!tar -zxvf caption_dataset.tar.gz\n","!tar -zxvf test_image.tar.gz"],"execution_count":3,"outputs":[{"output_type":"stream","text":["./caption_datasets/\n","./caption_datasets/TEST_IMAGES_flickr8k_processed.hdf5\n","./caption_datasets/VAL_IMAGES_flickr8k_processed.hdf5\n","./caption_datasets/VAL_CAPLENS_flickr8k_processed.json\n","./caption_datasets/VAL_CAPTIONS_flickr8k_processed.json\n","./caption_datasets/WORDMAP_flickr8k_processed.json\n","./caption_datasets/TEST_CAPLENS_flickr8k_processed.json\n","./caption_datasets/TEST_CAPTIONS_flickr8k_processed.json\n","./Flickr8k_Dataset/\n","./Flickr8k_Dataset/test4.jpg\n","./Flickr8k_Dataset/test5.jpg\n","./Flickr8k_Dataset/test3.jpg\n","./Flickr8k_Dataset/test1.jpg\n","./Flickr8k_Dataset/test2.jpg\n","./Flickr8k_Dataset/test7.jpg\n","./Flickr8k_Dataset/test10.jpg\n","./Flickr8k_Dataset/test8.jpg\n","./Flickr8k_Dataset/test6.jpg\n","./Flickr8k_Dataset/test9.jpg\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bn7nflRBuX7b","colab_type":"code","colab":{}},"source":["import time\n","import torch.backends.cudnn as cudnn\n","import torch.optim\n","import torch.utils.data\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import skimage.transform\n","from scipy.misc import imread, imresize\n","from helper.datasets import *\n","from helper.utils import *\n","from nltk.translate.bleu_score import corpus_bleu"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2bjVLdjuX7f","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \"\"\"\n","    Encoder.\n","    \"\"\"\n","\n","    def __init__(self, encoded_image_size=14):\n","        super(Encoder, self).__init__()\n","        self.enc_image_size = encoded_image_size\n","\n","        # load ResNet-1010 pretrained with ImageNet\n","        resnet = torchvision.models.resnet101(pretrained=True) \n","\n","        # Remove linear and pooling layers (not classification)\n","        modules = list(resnet.children())[:-2]\n","        self.resnet = nn.Sequential(*modules)\n","        \n","        # Resize the output image to fixed size to allow images of variable size\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n","\n","        self.fine_tune()\n","\n","    def forward(self, images):\n","        out = self.resnet(images) # (batch_size, 2048, image_size/32, image_size/32)\n","        out = self.adaptive_pool(out) # (batch_size, 2048, encoded_image_size, encoded_image_size)\n","        out = out.permute(0,2,3,1) # (batch_size, encoded_image_size, encoded_image_size, 2048)\n","        return out\n","\n","    def fine_tune(self, fine_tune=True):\n","        # we will be finetuning convolutional blocks 2~4 of the encoder\n","        for p in self.resnet.parameters(): # we want to fix most of the layers\n","          p.requires_grad = False\n","        for c in list(self.resnet.children())[5:]: # choose which layers to fine-tune\n","          for p in c.parameters():\n","            p.requires_grad = fine_tune # fine_true = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXz4OpETuX7h","colab_type":"code","colab":{}},"source":["class Attention(nn.Module):\n","    \"\"\"\n","    Attention Network.\n","    \"\"\"\n","\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        \"\"\"\n","        :param encoder_dim: feature size of encoded images\n","        :param decoder_dim: size of decoder's RNN\n","        :param attention_dim: size of the attention network\n","        TODO\n","        \"\"\"\n","        super(Attention, self).__init__()\n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim) # encoded image -> attention layer input\n","        self.decoder_att = nn.Linear(decoder_dim, attention_dim) # decoded output -> attention input\n","        self.full_att = nn.Linear(attention_dim, 1) # linear layer to calculate values to be softmax-ed\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, encoder_out, decoder_hidden):\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n","        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n","        :return: attention weighted encoding, weights\n","        TODO\n","        \"\"\"\n","        att1 = self.encoder_att(encoder_out) # (batch_size, num_pixels, attention_dim) \n","        att2 = self.decoder_att(decoder_hidden) # (batch_size, attention_dim)\n","        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n","        alpha = self.softmax(att) # (batch_size, num_pixels)\n","        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1) # (batch_size, encoder_dim) )\n","\n","        return attention_weighted_encoding, alpha"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4f6-lubIuX7k","colab_type":"code","colab":{}},"source":["class DecoderWithAttention(nn.Module):\n","    \"\"\"\n","    Decoder.\n","    \"\"\"\n","\n","    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n","        \"\"\"\n","        :param attention_dim: size of attention network\n","        :param embed_dim: embedding size\n","        :param decoder_dim: size of decoder's RNN\n","        :param vocab_size: size of vocabulary\n","        :param encoder_dim: feature size of encoded images\n","        :param dropout: dropout\n","        TODO\n","        \"\"\"\n","        super(DecoderWithAttention, self).__init__()\n","\n","        self.encoder_dim = encoder_dim\n","        self.attention_dim = attention_dim\n","        self.embed_dim = embed_dim\n","        self.decoder_dim = decoder_dim\n","        self.vocab_size = vocab_size\n","        self.dropout = dropout\n","\n","        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_dim) \n","        self.dropout = nn.Dropout(p=self.dropout) \n","        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n","        self.init_h = nn.Linear(encoder_dim, decoder_dim) # for initializing hidden state\n","        self.init_c = nn.Linear(encoder_dim, decoder_dim) # for initializing cell state\n","        self.f_beta = nn.Linear(decoder_dim, encoder_dim) # for creating a sigmoid-activated gate\n","        self.sigmoid = nn.Sigmoid()\n","        self.fc = nn.Linear(decoder_dim, vocab_size) # find probability over the whole vocab map\n","\n","        self.init_weights() # embedding, fc layer (bias + parameter) -> uniform initialization\n","\n","    def init_weights(self):\n","        \"\"\"\n","        Initializes some parameters with values from the uniform distribution, for easier convergence.\n","        CNN: weight initialization (ex) He Initialization)\n","        TODO\n","        \"\"\"\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","        self.fc.bias.data.fill_(0) \n","        self.fc.weight.data.uniform_(-0.1, 0.1)\n","\n","    def load_pretrained_embeddings(self, embeddings):\n","        \"\"\"\n","        Loads embedding layer with pre-trained embeddings.\n","\n","        :param embeddings: pre-trained embeddings\n","        TODO\n","        \"\"\"\n","        self.embedding.weight = nn.Parameter(embeddings)\n","\n","    def fine_tune_embeddings(self, fine_tune=True):\n","        \"\"\"\n","        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n","\n","        :param fine_tune: Allow?\n","        TODO\n","        \"\"\"\n","        for p in self.embedding.paramters():\n","          p.requires_grad = fine_tune\n","\n","    def init_hidden_state(self, encoder_out):\n","        \"\"\"\n","        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n","\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n","        :return: hidden state, cell state\n","        TODO\n","        \"\"\"\n","        mean_encoder_out = encoder_out.mean(dim=1) # (batch_size, encoder_dim) \n","        h = self.init_h(mean_encoder_out) # (batch_size, decoder_dim)\n","        c = self.init_c(mean_encoder_out) # (batch_size, decoder_dim) \n","        return h, c\n","\n","    def forward(self, encoder_out, encoded_captions, caption_lengths):\n","        \"\"\"\n","        Forward propagation.\n","\n","        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n","        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n","        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n","        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n","        TODO\n","        \"\"\"\n","        batch_size = encoder_out.size(0)\n","        encoder_dim = encoder_out.size(-1) \n","        vocab_size = self.vocab_size\n","\n","        # Image flattening\n","        encoder_out = encoder_out.view(batch_size, -1, encoder_dim) # (batch_size, num_pixels, encoder_dim)\n","        num_pixels = encoder_out.size(1)\n","\n","        # sort input data by decreasing lenghts\n","        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n","        encoder_out = encoder_out[sort_ind]\n","        encoded_captions = encoded_captions[sort_ind]\n","\n","        # Embedding\n","        embeddings = self.embedding(encoded_captions) # (batch_size, max_caption_length, emb_dim)\n","\n","        # Intialize hidden states\n","        h, c = self.init_hidden_state(encoder_out) # (batch_size, decoder_dim)\n","\n","        # we don't want to decode <end>\n","        # real decoding length  = caption_lengths - 1\n","        decode_lengths = (caption_lengths -1).tolist()\n","\n","        # tensors to store predicon scores and alphas (= attention weight)\n","        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n","        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n","\n","        # at each time step, decode by\n","        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n","        # then generate the next word with the previous word and attention weighed encoding\n","        for t in range(max(decode_lengths)):\n","            batch_size_t = sum([l > t for l in decode_lengths])\n","            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n","                                                                h[:batch_size_t])\n","            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n","            attention_weighted_encoding = gate * attention_weighted_encoding\n","            h, c = self.decode_step(\n","                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n","                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n","            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n","            predictions[:batch_size_t, t, :] = preds\n","            alphas[:batch_size_t, t, :] = alpha\n","\n","        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n","        \"\"\"\n","        for t in range(max(decode_lengths)):\n","            batch_size_t = sum([l > t for l in decode_lengths])\n","            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n","                                                                h[:batch_size_t])\n","            gate = self.sigmoid(self.f_beta(h[:batch_size_t])) # gating\n","            attention_weighted_encoding = gate * attention_weighted_encoding\n","            h, c = self.decode_step(\n","                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n","                (h[:batch_size_t], c[:batch_size_t])) # (batch_size_t, decode_dim)\n","            preds = self.fc( self.dropout(h)) # (batch_size_t, vocab_size)\n","            predictions[:batch_size_t, t, :] = preds\n","            alphas[:batch_size_t, t, :] = alpha\n","          return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n","          \"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1PugXXQuX7m","colab_type":"code","colab":{}},"source":["# Data parameters\n","data_folder = './caption_datasets/'  # folder with data files saved by create_input_files.py\n","data_name = 'flickr8k_processed'  # base name shared by data files\n","\n","# Model parameters\n","emb_dim = 512  # dimension of word embeddings\n","attention_dim = 512  # dimension of attention linear layers\n","decoder_dim = 512  # dimension of decoder RNN\n","dropout = 0.5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n","\n","# Training parameters\n","start_epoch = 0\n","epochs = 10  # number of epochs to train for (if early stopping is not triggered)\n","epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n","batch_size = 32\n","workers = 1  # for data-loading; right now, only 1 works with h5py\n","encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n","decoder_lr = 4e-4  # learning rate for decoder\n","grad_clip = 5.  # clip gradients at an absolute value of\n","alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n","best_bleu4 = 0.  # BLEU-4 score right now\n","print_freq = 100  # print training/validation stats every __ batches\n","fine_tune_encoder = False  # fine-tune encoder?\n","checkpoint = None  # path to checkpoint, None if none"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"svBdw9AxuX7p","colab_type":"code","colab":{}},"source":["def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n","    \"\"\"\n","    Performs one epoch's training.\n","\n","    :param train_loader: DataLoader for training data\n","    :param encoder: encoder model\n","    :param decoder: decoder model\n","    :param criterion: loss layer\n","    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n","    :param decoder_optimizer: optimizer to update decoder's weights\n","    :param epoch: epoch number\n","    TODO\n","    \"\"\"\n","    encoder.train()\n","    decoder.train()\n","\n","    batch_time = AverageMeter() # forward + back prop time\n","    data_time = AverageMeter() # data loading time\n","    losses = AverageMeter()\n","    top5accs = AverageMeter()\n","\n","    start = time.time()\n","\n","    # batch -> model\n","    for i, (imgs, caps, caplens, _) in enumerate(train_loader):\n","      data_time. update(time.time() - start)\n","\n","      # move to GPU\n","      imgs = imgs.to(device)\n","      caps = caps.to(device)\n","      caplens = caplens.to(device)\n","\n","      # Forward Prop\n","      imgs = encoder(imgs) # encoder output\n","      scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens) # decoder output\n","\n","      targets = caps_sorted[:, 1:]\n","\n","      # pad timesteps that were not decoded\n","      scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n","      targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n","\n","      # calculate loss\n","      loss = criterion(scores[0], targets[0])\n","\n","      # Add double stochastic attention regularization\n","      loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n","      #loss += alpha_c * ((1.-alphas.sum(dim=1)) ** 2)).mean()\n","\n","      # back prop\n","      decoder_optimizer.zero_grad()\n","      encoder_optimizer.zero_grad()\n","\n","      loss.backward()\n","\n","      # gradient clipping\n","      clip_gradient(decoder_optimizer, grad_clip)\n","      clip_gradient(encoder_optimizer, grad_clip)\n","\n","      # update weights\n","      decoder_optimizer.step()\n","      encoder_optimizer.step()\n","\n","      # update metrics\n","      top5 = accuracy(scores[0], targets[0], 5)\n","      losses.update(lossitem(), sum(decode_lengths))\n","      topaccs.update(top5, sum(decode_lengths))\n","      batch_time.update(time.time() - start)\n","\n","      start = time.time()\n","        # Print status\n","      if i % print_freq == 0:\n","          print('Epoch: [{0}][{1}/{2}]\\t'\n","                'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n","                                                                        batch_time=batch_time,\n","                                                                        data_time=data_time, loss=losses,\n","                                                                        top5=top5accs))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzMnHSUjuX7s","colab_type":"code","colab":{}},"source":["def validate(val_loader, encoder, decoder, criterion):\n","    \"\"\"\n","    Performs one epoch's validation.\n","\n","    :param val_loader: DataLoader for validation data.\n","    :param encoder: encoder model\n","    :param decoder: decoder model\n","    :param criterion: loss layer\n","    :return: BLEU-4 score\n","    \"\"\"\n","    decoder.eval()  # eval mode (no dropout or batchnorm)\n","    if encoder is not None:\n","        encoder.eval()\n","\n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    top5accs = AverageMeter()\n","\n","    start = time.time()\n","\n","    references = list()  # references (true captions) for calculating BLEU-4 score\n","    hypotheses = list()  # hypotheses (predictions)\n","\n","    # explicitly disable gradient calculation to avoid CUDA memory error\n","    # solves the issue #57\n","    with torch.no_grad():\n","        # Batches\n","        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n","\n","            # Move to device, if available\n","            imgs = imgs.to(device)\n","            caps = caps.to(device)\n","            caplens = caplens.to(device)\n","\n","            # Forward prop.\n","            if encoder is not None:\n","                imgs = encoder(imgs)\n","            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n","\n","            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n","            targets = caps_sorted[:, 1:]\n","\n","            # Remove timesteps that we didn't decode at, or are pads\n","            # pack_padded_sequence is an easy trick to do this\n","            scores_copy = scores.clone()\n","            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n","            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n","\n","            # Calculate loss\n","            loss = criterion(scores[0], targets[0])\n","\n","            # Add doubly stochastic attention regularization\n","            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n","\n","            # Keep track of metrics\n","            losses.update(loss.item(), sum(decode_lengths))\n","            top5 = accuracy(scores[0], targets[0], 5)\n","            top5accs.update(top5, sum(decode_lengths))\n","            batch_time.update(time.time() - start)\n","\n","            start = time.time()\n","\n","            if i % print_freq == 0:\n","                print('Validation: [{0}/{1}]\\t'\n","                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n","                                                                                loss=losses, top5=top5accs))\n","\n","            # Store references (true captions), and hypothesis (prediction) for each image\n","            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n","            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n","\n","            # References\n","            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n","            for j in range(allcaps.shape[0]):\n","                img_caps = allcaps[j].tolist()\n","                img_captions = list(\n","                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n","                        img_caps))  # remove <start> and pads\n","                references.append(img_captions)\n","\n","            # Hypotheses\n","            _, preds = torch.max(scores_copy, dim=2)\n","            preds = preds.tolist()\n","            temp_preds = list()\n","            for j, p in enumerate(preds):\n","                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n","            preds = temp_preds\n","            hypotheses.extend(preds)\n","\n","            assert len(references) == len(hypotheses)\n","\n","        # Calculate BLEU-4 scores\n","        bleu4 = corpus_bleu(references, hypotheses)\n","\n","        print(\n","            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n","                loss=losses,\n","                top5=top5accs,\n","                bleu=bleu4))\n","\n","    return bleu4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOvrSgTXuX7u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"outputId":"35561c6c-c675-48d7-b9c5-6760cfa30d5f","executionInfo":{"status":"error","timestamp":1582528884742,"user_tz":-540,"elapsed":468442,"user":{"displayName":"Jenny Mok","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBx6WjQhn61jI2o2RUoma2WrRvQIP2K7I8ROHvF7A=s64","userId":"14608516416463645410"}}},"source":["global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n","\n","# Read word map\n","word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n","with open(word_map_file, 'r') as j:\n","    word_map = json.load(j)\n","\n","# Initialize / load checkpoint\n","if checkpoint is None:\n","    decoder = DecoderWithAttention(attention_dim=attention_dim,\n","                                   embed_dim=emb_dim,\n","                                   decoder_dim=decoder_dim,\n","                                   vocab_size=len(word_map),\n","                                   dropout=dropout)\n","    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n","                                         lr=decoder_lr)\n","    encoder = Encoder()\n","    encoder.fine_tune(fine_tune_encoder)\n","    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n","                                         lr=encoder_lr) if fine_tune_encoder else None\n","\n","else:\n","    checkpoint = torch.load(checkpoint)\n","    start_epoch = checkpoint['epoch'] + 1\n","    epochs_since_improvement = checkpoint['epochs_since_improvement']\n","    best_bleu4 = checkpoint['bleu-4']\n","    decoder = checkpoint['decoder']\n","    decoder_optimizer = checkpoint['decoder_optimizer']\n","    encoder = checkpoint['encoder']\n","    encoder_optimizer = checkpoint['encoder_optimizer']\n","    if fine_tune_encoder is True and encoder_optimizer is None:\n","        encoder.fine_tune(fine_tune_encoder)\n","        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n","                                             lr=encoder_lr)\n","\n","# Move to GPU, if available\n","decoder = decoder.to(device)\n","encoder = encoder.to(device)\n","\n","# Loss function\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","# Custom dataloaders\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","train_loader = torch.utils.data.DataLoader(\n","    CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n","    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n","val_loader = torch.utils.data.DataLoader(\n","    CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n","    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n","\n","# Epochs\n","for epoch in range(start_epoch, epochs):\n","\n","    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n","    if epochs_since_improvement == 20:\n","        break\n","    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n","        adjust_learning_rate(decoder_optimizer, 0.8)\n","        if fine_tune_encoder:\n","            adjust_learning_rate(encoder_optimizer, 0.8)\n","\n","    # One epoch's training\n","    train(train_loader=train_loader,\n","          encoder=encoder,\n","          decoder=decoder,\n","          criterion=criterion,\n","          encoder_optimizer=encoder_optimizer,\n","          decoder_optimizer=decoder_optimizer,\n","          epoch=epoch)\n","\n","    # One epoch's validation\n","    recent_bleu4 = validate(val_loader=val_loader,\n","                            encoder=encoder,\n","                            decoder=decoder,\n","                            criterion=criterion)\n","\n","    # Check if there was an improvement\n","    is_best = recent_bleu4 > best_bleu4\n","    best_bleu4 = max(recent_bleu4, best_bleu4)\n","    if not is_best:\n","        epochs_since_improvement += 1\n","        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n","    else:\n","        epochs_since_improvement = 0\n","\n","    # Save checkpoint\n","    save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n","                    decoder_optimizer, recent_bleu4, is_best)"],"execution_count":11,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-1f962c3f6ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m           \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m           epoch=epoch)\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# One epoch's validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-d21ff9e5d286>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch)\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;31m# back prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_grad'"]}]},{"cell_type":"code","metadata":{"id":"WL5y_YrwuX7x","colab_type":"code","colab":{}},"source":["def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n","    \"\"\"\n","    Reads an image and captions it with beam search.\n","\n","    :param encoder: encoder model\n","    :param decoder: decoder model\n","    :param image_path: path to image\n","    :param word_map: word map\n","    :param beam_size: number of sequences to consider at each decode-step\n","    :return: caption, weights for visualization\n","    \"\"\"\n","\n","    k = beam_size\n","    vocab_size = len(word_map)\n","\n","    # Read image and process\n","    img = imread(image_path)\n","    if len(img.shape) == 2:\n","        img = img[:, :, np.newaxis]\n","        img = np.concatenate([img, img, img], axis=2)\n","    img = imresize(img, (256, 256))\n","    img = img.transpose(2, 0, 1)\n","    img = img / 255.\n","    img = torch.FloatTensor(img).to(device)\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","    transform = transforms.Compose([normalize])\n","    image = transform(img)  # (3, 256, 256)\n","\n","    # Encode\n","    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n","    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n","    enc_image_size = encoder_out.size(1)\n","    encoder_dim = encoder_out.size(3)\n","\n","    # Flatten encoding\n","    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n","    num_pixels = encoder_out.size(1)\n","\n","    # We'll treat the problem as having a batch size of k\n","    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n","\n","    # Tensor to store top k previous words at each step; now they're just <start>\n","    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n","\n","    # Tensor to store top k sequences; now they're just <start>\n","    seqs = k_prev_words  # (k, 1)\n","\n","    # Tensor to store top k sequences' scores; now they're just 0\n","    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n","\n","    # Tensor to store top k sequences' alphas; now they're just 1s\n","    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n","\n","    # Lists to store completed sequences, their alphas and scores\n","    complete_seqs = list()\n","    complete_seqs_alpha = list()\n","    complete_seqs_scores = list()\n","\n","    # Start decoding\n","    step = 1\n","    h, c = decoder.init_hidden_state(encoder_out)\n","\n","    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n","    while True:\n","\n","        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n","\n","        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n","\n","        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n","\n","        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n","        awe = gate * awe\n","\n","        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n","\n","        scores = decoder.fc(h)  # (s, vocab_size)\n","        scores = F.log_softmax(scores, dim=1)\n","\n","        # Add\n","        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n","\n","        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n","        if step == 1:\n","            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n","        else:\n","            # Unroll and find top scores, and their unrolled indices\n","            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n","\n","        # Convert unrolled indices to actual indices of scores\n","        prev_word_inds = top_k_words / vocab_size  # (s)\n","        next_word_inds = top_k_words % vocab_size  # (s)\n","\n","        # Add new words to sequences, alphas\n","        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n","        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n","                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n","\n","        # Which sequences are incomplete (didn't reach <end>)?\n","        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n","                           next_word != word_map['<end>']]\n","        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n","\n","        # Set aside complete sequences\n","        if len(complete_inds) > 0:\n","            complete_seqs.extend(seqs[complete_inds].tolist())\n","            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n","            complete_seqs_scores.extend(top_k_scores[complete_inds])\n","        k -= len(complete_inds)  # reduce beam length accordingly\n","\n","        # Proceed with incomplete sequences\n","        if k == 0:\n","            break\n","        seqs = seqs[incomplete_inds]\n","        seqs_alpha = seqs_alpha[incomplete_inds]\n","        h = h[prev_word_inds[incomplete_inds]]\n","        c = c[prev_word_inds[incomplete_inds]]\n","        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n","        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n","        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n","\n","        # Break if things have been going on too long\n","        if step > 50:\n","            break\n","        step += 1\n","\n","    i = complete_seqs_scores.index(max(complete_seqs_scores))\n","    seq = complete_seqs[i]\n","    alphas = complete_seqs_alpha[i]\n","\n","    return seq, alphas"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOud-YpJuX7z","colab_type":"code","colab":{}},"source":["def visualize_att(image_path, seq, alphas, rev_word_map, smooth=True):\n","    \"\"\"\n","    Visualizes caption with weights at every word.\n","\n","    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n","\n","    :param image_path: path to image that has been captioned\n","    :param seq: caption\n","    :param alphas: weights\n","    :param rev_word_map: reverse word mapping, i.e. ix2word\n","    :param smooth: smooth weights?\n","    \"\"\"\n","    image = Image.open(image_path)\n","    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n","\n","    words = [rev_word_map[ind] for ind in seq]\n","\n","    for t in range(len(words)):\n","        if t > 50:\n","            break\n","        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n","\n","        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n","        plt.imshow(image)\n","        current_alpha = alphas[t, :]\n","        if smooth:\n","            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n","        else:\n","            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n","        if t == 0:\n","            plt.imshow(alpha, alpha=0)\n","        else:\n","            plt.imshow(alpha, alpha=0.8)\n","        plt.set_cmap(cm.Greys_r)\n","        plt.axis('off')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcEaTZvtuX72","colab_type":"code","colab":{}},"source":["img_dir = './Flickr8k_Dataset/test1.jpg'\n","model_dir = './BEST_checkpoint_flickr8k_processed.pth.tar'\n","wordmap_dir = './caption_datasets/WORDMAP_flickr8k_processed.json'\n","beam_size = 5\n","smooth = True\n","\n","# Load model\n","checkpoint = torch.load(model_dir)\n","decoder = checkpoint['decoder']\n","decoder = decoder.to(device)\n","decoder.eval()\n","encoder = checkpoint['encoder']\n","encoder = encoder.to(device)\n","encoder.eval()\n","\n","# Load word map (word2ix)\n","with open(wordmap_dir, 'r') as j:\n","    word_map = json.load(j)\n","rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n","\n","# Encode, decode with attention and beam search\n","seq, alphas = caption_image_beam_search(encoder, decoder, img_dir, word_map, beam_size)\n","alphas = torch.FloatTensor(alphas)\n","\n","# Visualize caption and attention of best sequence\n","visualize_att(img_dir, seq, alphas, rev_word_map, smooth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wPDpG3MuX74","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
