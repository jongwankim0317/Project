{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 2: Language Modeling with CharRNN\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Sangil Lee, October 2018, modified by Jungbeom Lee, October 2020.\n",
    "\n",
    "This is  a basic character-level RNN to classify words.\n",
    "\n",
    "A character-level RNN reads words as a series of characters - outputting a prediction and ?hidden state? at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we will train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling:\n",
    "\n",
    "\n",
    "Original blog post & code:\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "\n",
    "\n",
    "This iPython notebook is basically a copypasta of this repo.\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of RNNs more clearly in a code level.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1-3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your student number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Classifying words with character-level RNN (30 points)\n",
    "\n",
    "\n",
    "1. Successful training through implementing code that works. You will need to implement the codes in char_rnn.py.  (15 points)\n",
    "\n",
    "\n",
    "2. After training, the final accuracy must be <font color=red> above 65% </font> (please see the last code block). We don't split the data into train-valid-test. Don't forget to <font color=red> NOT clear the outputs of all the code blocks! (15 points)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now proceed to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/German.txt', 'data/names/Arabic.txt', 'data/names/Vietnamese.txt', 'data/names/Korean.txt', 'data/names/Italian.txt', 'data/names/Japanese.txt', 'data/names/Dutch.txt', 'data/names/Greek.txt', 'data/names/English.txt', 'data/names/Polish.txt', 'data/names/Spanish.txt', 'data/names/Czech.txt', 'data/names/Irish.txt', 'data/names/Chinese.txt', 'data/names/Scottish.txt', 'data/names/Russian.txt', 'data/names/French.txt', 'data/names/Portuguese.txt']\n",
      "lusarski\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('lus√†rski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "\n",
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names to Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings for training and inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters, should not be changed.\n",
    "N_CATEGORIES = len(all_categories)\n",
    "N_LETTERS = len(all_letters)\n",
    "N_EVAL_SAMPLES_PER_CATEGORY = 10\n",
    "\n",
    "# Adjustable parameters. You can change these parameters freely.\n",
    "\n",
    "N_HIDDEN = 256\n",
    "LEARNING_RATE = 5e-2\n",
    "N_ITERS = 100000\n",
    "print_every = 1000\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Greek / line = Kanavos\n",
      "category = Scottish / line = Mackay\n",
      "category = Greek / line = Arvanitoyannis\n",
      "category = English / line = Charge\n",
      "category = Spanish / line = Garza\n",
      "category = French / line = Degarmo\n",
      "category = Russian / line = Vanzha\n",
      "category = Scottish / line = Jamieson\n",
      "category = Italian / line = Albrici\n",
      "category = Greek / line = Dertilis\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "def load_train_example(n_per_cls):\n",
    "    category_tensors, line_tensors = [], []\n",
    "    for category in all_categories:\n",
    "        for line in category_lines[category][:n_per_cls]:\n",
    "            category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "            line_tensor = lineToTensor(line)\n",
    "            category_tensors.append(category_tensor)\n",
    "            line_tensors.append(line_tensor)\n",
    "    \n",
    "    return category_tensors, line_tensors\n",
    "    \n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)\n",
    "\n",
    "category_tensors, line_tensors = load_train_example(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should NOT change this code block\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = torch.zeros(1, N_HIDDEN).cuda()\n",
    "    category_tensor = category_tensor.cuda()\n",
    "    line_tensor = line_tensor.cuda()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    output = rnn(line_tensor, hidden)\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-LEARNING_RATE)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "RNN(\n",
      "  (lstm): LSTM(57, 256)\n",
      "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "1000 1% (2.9002845287323) Favreau / Greek ? (French) 0.05\n",
      "2000 2% (2.9378175735473633) Battaglia / English ? (Italian) 0.05\n",
      "3000 3% (2.3564138412475586) Guthrie / English correct! 0.05\n",
      "4000 4% (2.685858726501465) Mandel / English ? (German) 0.05\n",
      "5000 5% (0.696927547454834) Zhilnikov / Russian correct! 0.05\n",
      "6000 6% (3.220208168029785) Kouba / Japanese ? (Czech) 0.05\n",
      "7000 7% (1.0778177976608276) Kerner / German correct! 0.05\n",
      "8000 8% (1.9001375436782837) Pei / Italian ? (Chinese) 0.05\n",
      "9000 9% (2.3661084175109863) Kloeten / Scottish ? (Dutch) 0.05\n",
      "10000 10% (0.03548244759440422) Moshkov / Russian correct! 0.05\n",
      "11000 11% (0.06728974729776382) Georgeakopoulos / Greek correct! 0.05\n",
      "12000 12% (1.558113932609558) Wan / Vietnamese ? (Chinese) 0.05\n",
      "13000 13% (1.355043888092041) Van / Korean ? (Vietnamese) 0.05\n",
      "14000 14% (1.33513605594635) Bustillo / Italian ? (Spanish) 0.05\n",
      "15000 15% (2.3199410438537598) White / English ? (Scottish) 0.05\n",
      "16000 16% (2.7788543701171875) Plisko / Polish ? (Czech) 0.05\n",
      "17000 17% (3.5431110858917236) Wyman / Irish ? (German) 0.05\n",
      "18000 18% (3.0013246536254883) Dale / Irish ? (Dutch) 0.05\n",
      "19000 19% (1.4686124324798584) Kouches / Greek correct! 0.05\n",
      "20000 20% (1.396803379058838) Watson / English ? (Scottish) 0.05\n",
      "21000 21% (1.3545513153076172) Reier / German correct! 0.05\n",
      "22000 22% (0.7856256365776062) Nicolai / Italian correct! 0.05\n",
      "23000 23% (0.8800204992294312) Konigsmann / Dutch ? (German) 0.05\n",
      "24000 24% (1.7679083347320557) Emms / Dutch ? (English) 0.05\n",
      "25000 25% (0.04582547768950462) Stavropoulos / Greek correct! 0.05\n",
      "26000 26% (1.6191662549972534) Herrera / Portuguese ? (Spanish) 0.05\n",
      "27000 27% (0.038487959653139114) Krakowski / Polish correct! 0.05\n",
      "28000 28% (2.9114856719970703) Daal / Arabic ? (Dutch) 0.05\n",
      "29000 28% (0.2645379900932312) Menendez / Spanish correct! 0.05\n",
      "30000 30% (1.4276275634765625) Monet / German ? (French) 0.05\n",
      "31000 31% (0.0041108159348368645) Komagata / Japanese correct! 0.05\n",
      "32000 32% (0.2589710056781769) Samaha / Arabic correct! 0.05\n",
      "33000 33% (1.7612868547439575) Saliba / Spanish ? (Arabic) 0.05\n",
      "34000 34% (0.7943149209022522) Zheng / Chinese correct! 0.05\n",
      "35000 35% (2.0240302085876465) Rompa / Italian ? (Dutch) 0.05\n",
      "36000 36% (1.892314076423645) Vliert / French ? (Dutch) 0.05\n",
      "37000 37% (0.2064839005470276) Masih / Arabic correct! 0.05\n",
      "38000 38% (0.015838533639907837) Missiakos / Greek correct! 0.05\n",
      "39000 39% (0.00410511763766408) Adamidis / Greek correct! 0.05\n",
      "40000 40% (0.004279740620404482) Adamidis / Greek correct! 0.05\n",
      "41000 41% (0.6230059266090393) Richard / French correct! 0.05\n",
      "42000 42% (0.0002982171718031168) Hamenkov / Russian correct! 0.05\n",
      "43000 43% (0.5019727945327759) Beaumont / French correct! 0.05\n",
      "44000 44% (0.05117056146264076) Pho / Vietnamese correct! 0.05\n",
      "45000 45% (3.0388190746307373) Young / Dutch ? (Scottish) 0.05\n",
      "46000 46% (4.920864582061768) Berger / German ? (French) 0.05\n",
      "47000 47% (0.059149716049432755) Truong / Vietnamese correct! 0.05\n",
      "48000 48% (0.020469725131988525) Kurohiko / Japanese correct! 0.05\n",
      "49000 49% (2.038208484649658) Seidel / Czech ? (German) 0.05\n",
      "50000 50% (1.7942947149276733) Lee / Chinese ? (Korean) 0.05\n",
      "51000 51% (0.5132977366447449) Williamson / Scottish correct! 0.05\n",
      "52000 52% (0.6937560439109802) Basara / Arabic correct! 0.05\n",
      "53000 53% (0.3754255771636963) Choe / Korean correct! 0.05\n",
      "54000 54% (0.024996206164360046) Lieu / Vietnamese correct! 0.05\n",
      "55000 55% (2.9202423095703125) Jamussa / Czech ? (Greek) 0.05\n",
      "56000 56% (0.02181393653154373) Zhai / Chinese correct! 0.05\n",
      "57000 56% (0.1444801688194275) Teagan / Irish correct! 0.05\n",
      "58000 57% (0.801082193851471) Luong / Chinese ? (Vietnamese) 0.05\n",
      "59000 59% (2.5919339656829834) Anwar / Arabic ? (English) 0.05\n",
      "60000 60% (1.9860914945602417) Bui / Chinese ? (Vietnamese) 0.05\n",
      "61000 61% (0.0850180983543396) Chao / Chinese correct! 0.05\n",
      "62000 62% (0.12070094048976898) Sutherland / Scottish correct! 0.05\n",
      "63000 63% (3.75531268119812) Janson / English ? (German) 0.05\n",
      "64000 64% (4.566154479980469) Groe / Scottish ? (German) 0.05\n",
      "65000 65% (0.33927881717681885) Borovsky / Czech correct! 0.05\n",
      "66000 66% (0.00766692403703928) Egonidis / Greek correct! 0.05\n",
      "67000 67% (0.39167651534080505) AuYong / Chinese correct! 0.05\n",
      "68000 68% (0.8145822286605835) Gray / Scottish correct! 0.05\n",
      "69000 69% (0.026986798271536827) Kasprzak / Polish correct! 0.05\n",
      "70000 70% (0.016775842756032944) Tokuoka / Japanese correct! 0.05\n",
      "71000 71% (0.6520935893058777) Suero / Spanish correct! 0.05\n",
      "72000 72% (0.02546836994588375) Meadhra / Irish correct! 0.05\n",
      "73000 73% (0.12425771355628967) Iwasa / Japanese correct! 0.05\n",
      "74000 74% (0.00793885625898838) Ribeiro / Portuguese correct! 0.05\n",
      "75000 75% (0.03174855187535286) Paschalis / Greek correct! 0.05\n",
      "76000 76% (0.3648530840873718) Mendez / Spanish correct! 0.05\n",
      "77000 77% (1.88229238986969) Bonaventura / Italian ? (Spanish) 0.05\n",
      "78000 78% (0.0056174276396632195) Terakado / Japanese correct! 0.05\n",
      "79000 79% (0.0001760566228767857) Grigorchikov / Russian correct! 0.05\n",
      "80000 80% (0.08836834132671356) Oaks / English correct! 0.05\n",
      "81000 81% (0.16783587634563446) Pasternak / Polish correct! 0.05\n",
      "82000 82% (0.27471157908439636) Ko / Korean correct! 0.05\n",
      "83000 83% (0.574508547782898) Bentley / English correct! 0.05\n",
      "84000 84% (0.060200877487659454) Kwang  / Korean correct! 0.05\n",
      "85000 85% (0.04970386624336243) Rhee / Korean correct! 0.05\n",
      "86000 86% (1.425256609916687) Mondo / Spanish ? (Italian) 0.05\n",
      "87000 87% (0.011023101396858692) Tupikov / Russian correct! 0.05\n",
      "88000 88% (0.028879977762699127) Chi / Korean correct! 0.05\n",
      "89000 89% (0.04147302731871605) Duchamps / French correct! 0.05\n",
      "90000 90% (0.005675274413079023) To / Vietnamese correct! 0.05\n",
      "91000 91% (0.060148682445287704) Mackenzie / Scottish correct! 0.05\n",
      "92000 92% (0.6353070139884949) Rios / Portuguese correct! 0.05\n",
      "93000 93% (0.5681788325309753) Herbert / German correct! 0.05\n",
      "94000 94% (0.0003361137059982866) Zhao / Chinese correct! 0.05\n",
      "95000 95% (0.11963611096143723) Baasch / German correct! 0.05\n",
      "96000 96% (0.6959104537963867) Davidson / Scottish correct! 0.05\n",
      "97000 97% (0.11855701357126236) Cantu / Italian correct! 0.05\n",
      "98000 98% (0.1224249079823494) Grozmanova / Czech correct! 0.05\n",
      "99000 99% (0.0006804534932598472) Kotara / Japanese correct! 0.05\n",
      "100000 100% (0.0048831491731107235) Kalaidovich / Russian correct! 0.05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from char_rnn import RNN\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_CATEGORIES).cuda()\n",
    "print(rnn)\n",
    "rnn.train()\n",
    "criterion = nn.NLLLoss()\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "for iter in range(1, N_ITERS + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = 'correct!' if guess == category else '? (%s)' % category\n",
    "        print('%d %d%% (%s) %s / %s %s %s' % (iter, iter / N_ITERS * 100, loss, line, guess, correct, LEARNING_RATE))\n",
    "torch.save(rnn.state_dict(), 'models/RNN_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval with 180 samples\n",
      "0.5305531033135923\n",
      "0.8611111111111112\n"
     ]
    }
   ],
   "source": [
    "## Should NOT change this code block\n",
    "category_tensors, line_tensors = load_train_example(N_EVAL_SAMPLES_PER_CATEGORY)\n",
    "\n",
    "total_loss = 0\n",
    "n_samples = 0\n",
    "n_correct = 0\n",
    "for idx in range(len(category_tensors)):\n",
    "    n_samples += 1\n",
    "    category_tensor, line_tensor = category_tensors[idx].cuda(), line_tensors[idx].cuda()\n",
    "    hidden = torch.zeros(1, N_HIDDEN).cuda()\n",
    "    \n",
    "    output = rnn(line_tensor, hidden)\n",
    "\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     print(guess_i, category_tensor)\n",
    "    if guess_i == category_tensor[0].data.cpu().numpy():\n",
    "        n_correct += 1\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "print(\"eval with %d samples\" % n_samples)\n",
    "print(total_loss / n_samples)\n",
    "print(n_correct / n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval with 180 samples\n",
      "0.5305531033135923\n",
      "0.8611111111111112\n"
     ]
    }
   ],
   "source": [
    "# check your saved checkpoint again\n",
    "\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_CATEGORIES).cuda()\n",
    "rnn.eval()\n",
    "rnn.load_state_dict(torch.load('models/RNN_1.pth'), strict=True)\n",
    "\n",
    "category_tensors, line_tensors = load_train_example(N_EVAL_SAMPLES_PER_CATEGORY)\n",
    "\n",
    "total_loss = 0\n",
    "n_samples = 0\n",
    "n_correct = 0\n",
    "for idx in range(len(category_tensors)):\n",
    "    n_samples += 1\n",
    "    category_tensor, line_tensor = category_tensors[idx].cuda(), line_tensors[idx].cuda()\n",
    "    hidden = torch.zeros(1, N_HIDDEN).cuda()\n",
    "    \n",
    "    output = rnn(line_tensor, hidden)\n",
    "\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     print(guess_i, category_tensor)\n",
    "    if guess_i == category_tensor[0].data.cpu().numpy():\n",
    "        n_correct += 1\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "print(\"eval with %d samples\" % n_samples)\n",
    "print(total_loss / n_samples)\n",
    "print(n_correct / n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep20] *",
   "language": "python",
   "name": "conda-env-deep20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
